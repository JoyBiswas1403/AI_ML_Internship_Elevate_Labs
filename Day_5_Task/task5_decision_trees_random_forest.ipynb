# Name: Joy Biswas
# Task 5: Decision Trees and Random Forests â€“ Heart Failure Clinical Dataset

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 1. Set up and load the data
os.makedirs('screenshots', exist_ok=True)
url = 'https://raw.githubusercontent.com/anik199/Heart-failure-prediction/main/heart.csv'  # 918 samples, 12 features
df = pd.read_csv(url)

# 2. Encode categorical features if needed
df['Sex'] = df['Sex'].map({'M': 1, 'F': 0})
df['ExerciseAngina'] = df['ExerciseAngina'].map({'Y': 1, 'N': 0})
df = pd.get_dummies(df, columns=['ChestPainType', 'RestingECG', 'ST_Slope'], drop_first=True)

# 3. Features & Target
X = df.drop('HeartDisease', axis=1)
y = df['HeartDisease']

# 4. Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2)

# 5. Decision Tree Classifier (baseline, default depth)
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)
acc_dt = accuracy_score(y_test, y_pred_dt)
print(f"Decision Tree Test Accuracy: {acc_dt:.3f}")

# 6. Control Overfitting: Restrict tree depth
dt3 = DecisionTreeClassifier(max_depth=3, random_state=42)
dt3.fit(X_train, y_train)
y_pred_dt3 = dt3.predict(X_test)
acc_dt3 = accuracy_score(y_test, y_pred_dt3)
print(f"Decision Tree (max_depth=3) Test Accuracy: {acc_dt3:.3f}")

# Visualization of the restricted tree
from sklearn.tree import plot_tree
plt.figure(figsize=(20,7))
plot_tree(dt3, feature_names=X.columns, class_names=['No Disease', 'Disease'], filled=True)
plt.tight_layout()
plt.savefig('screenshots/decision_tree.png')
plt.close()

# 7. Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
acc_rf = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest (max_depth=5) Test Accuracy: {acc_rf:.3f}")

# 8. Cross-Validation Comparison
cv_dt = cross_val_score(dt3, X, y, cv=5).mean()
cv_rf = cross_val_score(rf, X, y, cv=5).mean()
print(f"Decision Tree CV Accuracy: {cv_dt:.3f}")
print(f"Random Forest CV Accuracy: {cv_rf:.3f}")

# 9. Feature Importance
importances = rf.feature_importances_
fi_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances}).sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=fi_df)
plt.title('Random Forest Feature Importances')
plt.tight_layout()
plt.savefig('screenshots/feature_importance.png')
plt.close()

print("\nRandom Forest Most Important Features:")
print(fi_df.head())

# 10. Confusion Matrix for Random Forest
cm = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(4,3))
sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', cbar=False)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Random Forest Confusion Matrix")
plt.tight_layout()
plt.savefig('screenshots/confusion_matrix.png')
plt.close()

# 11. Save cleaned dataset (with encoded features, as used for modeling)
df.to_csv('cleaned_heart_failure.csv', index=False)

# Download code for Colab or Jupyter (uncomment these in Colab)
from google.colab import files
files.download('cleaned_heart_failure.csv')
files.download('screenshots/decision_tree.png')
files.download('screenshots/feature_importance.png')
files.download('screenshots/confusion_matrix.png')
